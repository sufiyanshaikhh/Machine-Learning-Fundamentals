{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8effc0ce-0f9b-4aca-8559-5719a4b8dea6",
   "metadata": {},
   "source": [
    "## Regularization in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa1e970-851b-44e0-9c09-b0d4f2b72098",
   "metadata": {},
   "source": [
    "Extending the previous linear and logistic cost functions routines and re - implementing with a regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009a82ca-3fbc-427b-9b3f-e696442a07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dependencies\n",
    "import numpy as np  # For scientific computation\n",
    "import matplotlib.pyplot as plt  # For plotting data\n",
    "from lab_utils_common import sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2366f2bb-e75e-447c-942e-db9492e255ef",
   "metadata": {},
   "source": [
    "Including regularization encourages gradient descent to minimize the size of the parameters. Here, the parameter b is not regularized. This is standard practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d11e8f-d712-4441-a1bc-fcddcb6827ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cost but with regularization for linear regression\n",
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m  = X.shape[0]\n",
    "    n  = len(w)\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n",
    "        cost = cost + (f_wb_i - y[i])**2                               #scalar             \n",
    "    cost = cost / (2 * m)                                              #scalar  \n",
    " \n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    return total_cost                                                  #scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8f7f05-f1ef-4535-8917-d537e861f859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.07917239320214275\n"
     ]
    }
   ],
   "source": [
    "# Implementing regularized cost function\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4fce87c-d1aa-4ad2-ba12-f1fcfea8d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cost but with regularization for logistic regression\n",
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m,n  = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n",
    "        f_wb_i = sigmoid(z_i)                                          #scalar\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
    "             \n",
    "    cost = cost/m                                                      #scalar\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    return total_cost                                                  #scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808188f5-9d22-415b-b9d5-67822c986559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.6850849138741673\n"
     ]
    }
   ],
   "source": [
    "# Implementing regularized cost function\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f9d245-175a-4a00-bdb9-e9d3364175ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient function for regularized linear regression\n",
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m   \n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e2d6d90-d16e-462b-9c0d-cbcc97e05687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.6648774569425726\n",
      "Regularized dj_dw:\n",
      " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n"
     ]
    }
   ],
   "source": [
    "# Implementing the gradient for regularized linear regression\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb26b89-62f7-43d5-ba8f-8b91c1b3165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient function for regularized logistic regression\n",
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            #(n,)\n",
    "    dj_db = 0.0                                       #scalar\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cccb6d6-9cbb-4fef-860b-1d06bb814ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.341798994972791\n",
      "Regularized dj_dw:\n",
      " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n"
     ]
    }
   ],
   "source": [
    "# Implementing the gradient for regularized logistic regression\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6e377-b42f-4fc3-a02a-196cda26bae4",
   "metadata": {},
   "source": [
    "## Regularization Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760ebe1-fb0f-4d1d-8713-a94d19d4ce01",
   "metadata": {},
   "source": [
    "Adding regularization does not inherently increase the number of iterations. It modifies the objective function by adding an additional gradient term that biases parameters toward smaller values. The gradient is not smaller; it is different. Convergence speed depends on optimization dynamics, learning rate, and conditioning — and regularization can even improve stability and convergence in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffcf3f8-c886-4347-8a7f-80fd719ddc17",
   "metadata": {},
   "source": [
    "Regularization does not shift the gradient toward the old minimum; it modifies the objective function, altering the entire gradient field and moving the optimal solution to a new location, typically closer to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e1d47-9ae9-4676-95a3-afc4cb917eb6",
   "metadata": {},
   "source": [
    "Regularization doesn’t add more minima — it changes the landscape so that the best trade-off between data fit and model simplicity occurs at a different point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
