{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0f3a3a-b4c1-4416-bb08-40471169e3df",
   "metadata": {},
   "source": [
    "## Overfitting in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a4180-c855-4a11-bcb6-593c82191790",
   "metadata": {},
   "source": [
    "Overfitting happens when a machine learning model learns the training data too well, including its noise and random fluctuations, and as a result performs poorly on new, unseen data. In such a case, model performs very well on the training data, but fails to perform on test data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1e175-fd9a-4727-a2d7-40c25202b6fb",
   "metadata": {},
   "source": [
    "For examples, in house price prediction, the model learns the pattern in the training data too well that is it fits each and every training point resulting in a complex curve with higher degree of polynomials. Such a model when predicts the price of an unseen house, the prediction is completely wrong. The model learned patterns specific to training data, not general rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088a1c4-55ad-4e42-a0a4-aeed4683c81f",
   "metadata": {},
   "source": [
    "## Causes of Overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2095ab-bab3-49a1-8f71-c1576a8ad5e7",
   "metadata": {},
   "source": [
    "Few causes of overfitting, when the curve is too complex and fits each and every training point int the dataset (higher degree of polynomials),\n",
    "the training sample is too less, there are way more features in the training data, trained the model for a long time (ran gradient descent for too many iterations), the training sample includes too many noises to track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091da502-8dd9-47c9-894e-67eb01ba68cc",
   "metadata": {},
   "source": [
    "## Ways to detect overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ef660-0888-460c-a00e-43ba13b431ac",
   "metadata": {},
   "source": [
    "The loss calculated on the training data is nearly minimum, but when computed on validation, it appears to be way opposite. In such cases, the models accuracy on training data and testing data differs alot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5200c-d600-4981-af41-8dda032f1589",
   "metadata": {},
   "source": [
    "## Ways to prevent overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968b49a-be05-40d4-9e95-a0e1bc2a9509",
   "metadata": {},
   "source": [
    "Few ways to prevent overfitting, the most natural solution would be to train the model on a large dataset, another way is to include only relevant features and remove the irrelevant ones, this naturally reduces the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53d467-ac98-4528-988c-b74194c2ad84",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3971c5b-a687-431a-9f47-276abf6ddb68",
   "metadata": {},
   "source": [
    "The most efficient technique to reduce overfitting is regularization. Here, We penalizes larger weights such that their effect on the curve is close to minimum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
