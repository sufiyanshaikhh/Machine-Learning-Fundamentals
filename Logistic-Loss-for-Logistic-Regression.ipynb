{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66755d2-94b7-4ef4-a25f-a32b92e83b07",
   "metadata": {},
   "source": [
    "## Logistic loss for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e3435-c38c-4027-aab0-2f3ab0fafd84",
   "metadata": {},
   "source": [
    "Visualizing the reason why squared error cost function is not appropriate for logistic regression and exploring the new logistic loss for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9192ee9-3cba-4be4-8a2f-8e9f31c259ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dependencies\n",
    "import numpy as np  # For scientific computation\n",
    "import matplotlib.pyplot as plt  # For plotting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3512b4-faa3-430f-a1e7-96123ba12a35",
   "metadata": {},
   "source": [
    "The squared error cost function worked very well for the linear regression. This is because it produces a smooth surface plot of the cost function J(w, b). This smooth surface allows gradient descent to follow the derivative in order to reach the global minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc968ae-02cd-4849-8a1d-e51b1871d5d2",
   "metadata": {},
   "source": [
    "However, the SQE cost produces a very wiggly cost surface for logistic regression. This is mainly because, now there's a non linear component (the sigmoid function). The gradient descent is the case faces difficulty to reach the global minimum and gets trapped at various local minimums."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236acc1-f356-48be-b5f6-09b1efc6206b",
   "metadata": {},
   "source": [
    "Thus, we need to derive a new loss function to compute cost across all the training examples in the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3f4e9-c0d7-4aa6-9a8f-bb98169f9e1e",
   "metadata": {},
   "source": [
    "## Logistic loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b3a0f-772e-4ce7-8785-07cb9024c2dc",
   "metadata": {},
   "source": [
    "Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. Here, Loss is a measure of the difference of a single example to its target value while the Cost is a measure of the losses over the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e71910-85f6-4ba4-b69d-b3b772f69926",
   "metadata": {},
   "source": [
    "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or (y = 0) and another for when the target is one (y = 1). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a269530c-699e-4256-872e-40a11557a906",
   "metadata": {},
   "source": [
    "The curve of the surface plot is well suited to gradient descent. It does not have plateaus, local minima, or discontinuities. It is not a bowl as in the case of squared error. Both the cost and the log of the cost illuminate the fact that the curve, when the cost is small, has a slope and continues to decline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
